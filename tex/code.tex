\chapter{课题主要代码}
\section{主函数}
\begin{lstlisting}[language={python}]
import copy
import PIL.Image as Image
import numpy as np
from dueling.dueling_brain import Brain
from env.gym_super_mario_bros import env
from utils.img import get_gif


def RGB2gray(obs):
    img = Image.fromarray(obs).crop((0, 40, 256, 240)).resize((200, 200))
    img = img.convert('L')
    return np.asarray(img)


if __name__ == '__main__':
    state = env.reset()
    w, h, deep = state.shape
    state = RGB2gray(state)
    frame_len = 4
    memory_size = 1000000
    brain = Brain(memory_size=memory_size,
                    input_args=frame_len,
                    num_actions=7,
                    shape=state.shape,
                    learning_rate=0.00025,
                    reward_decay=0.99,
                    e_greedy=0.95,
                    e_greedy_increment=0.0001,
                    e_greedy_start=0,
                    batch_size=32,
                    replace_target_iter=10000)
    brain.store_start_frame(state)
    for i in range(memory_size + 5):
        print('\r', i)
        action = env.action_space.sample()
        obs, re, done, info = env.step(action)
        obs = RGB2gray(obs)
        env.render()
        re /= 15.0
        brain.store_transition(reward=re, action=action, obs_=obs)
        if done:
            env.reset()
    step = 1
    last_info = env.unwrapped._get_info()
    recording = []
    reward_list = []
    r = 0
    while step < 40000000:
        last_frame = brain.get_last_memory()
        action = brain.choose_action(last_frame)
        obs_, re, done, info = env.step(action)
        recording.append(copy.deepcopy(obs_))
        if done:
            if last_info['world'] == 2:
                get_gif(recording, step)
                recording = []
            obs_ = env.reset()
            reward_list.append(r)
            r = 0
        obs_ = RGB2gray(obs_)
        env.render()
        reward = re / 15.0
        r += reward
        print(action, reward, brain.epsilon, step)
        if reward < -0.6:
            print(reward)
            print(last_info)
            print(info)

        brain.store_transition(reward=reward, action=action, obs_=copy.deepcopy(obs_))
        last_info = info
        if step % 4 == 0:
            brain.double_learn()
        if step % 10000 == 0:
            np.save('./npy/' + str(step) + '.npy', np.array(reward_list))
            reward_list.clear()
        step += 1
\end{lstlisting}
\section{学习、决策中心}
\begin{lstlisting}[language={python}]
import os

import numpy as np
import torch
import torch.nn as nn
from torch.autograd import Variable

from dueling.memory import Memory
from dueling.model import DuelingCNN as CNN

save_q_eval_path = 'dueling/save_pkl/q_eval.pkl'
save_q_next_path = 'dueling/save_pkl/q_next.pkl'

USE_CUDA = torch.cuda.is_available()
if torch.cuda.is_available():
    dtype = torch.cuda.FloatTensor
else:
    dtype = torch.FloatTensor


class AbsErrorLoss(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, q_target, q_eval):
        return torch.sum(torch.abs(q_target - q_target), dim=1)


class Loss(nn.Module):
    def __init__(self):
        super().__init__()
        self.loss = nn.MSELoss(reduce=False, reduction='none')

    def forward(self, IS_weights, q_target, q_eval):
        x = self.loss(q_target, q_eval)
        return torch.mean(IS_weights * x)


class Variable(Variable):
    def __init__(self, data, *args, **kwargs):
        if USE_CUDA:
            data = data.cuda()
        super(Variable, self).__init__(data, *args, **kwargs)


class Brain:
    def __init__(self,
                    memory_size,
                    input_args,
                    num_actions,
                    shape,
                    learning_rate,
                    reward_decay,
                    e_greedy,
                    e_greedy_increment,
                    e_greedy_start,
                    batch_size,
                    replace_target_iter):
        self.q_eval = CNN(in_channels=input_args, num_action=num_actions).type(dtype)
        self.q_next = CNN(in_channels=input_args, num_action=num_actions).type(dtype)
        if os.path.isfile(save_q_eval_path):
            print('load q_eval ...')
            self.q_eval.load_state_dict(torch.load(save_q_eval_path))
        if os.path.isfile(save_q_next_path):
            print('load q_next ...')
            self.q_next.load_state_dict(torch.load(save_q_next_path))
        self.memory = Memory(size=memory_size, frame_len=input_args, w=shape[0], h=shape[1])
        self.channels = input_args
        self.num_action = num_actions
        self.learning_rate = learning_rate
        self.gamma = reward_decay
        self.batch_size = batch_size
        self.replace_target_iter = replace_target_iter
        self.epsilon_max = e_greedy
        self.epsilon_increment = e_greedy_increment
        self.epsilon = e_greedy_start
        self.learn_step_count = 0

        self.op = torch.optim.Adam(self.q_eval.parameters(), lr=learning_rate)  # optimize all cnn parameters
        self.loss_func = Loss()
        self.abs_errors_func = AbsErrorLoss()
        self.save_step = 10000
        self.learn_step = 0

    def choose_action(self, obs):
        if np.random.uniform() < self.epsilon and obs.shape[0] == self.channels:
            obs = torch.FloatTensor(obs)
            obs = obs.unsqueeze(0).type(dtype)
            out = self.q_eval(obs / 255.0)
            return np.argmax(out.detach().cpu()).item()
        return np.random.randint(0, self.num_action - 1)

    def store_transition(self, action, reward, obs_):
        self.memory.store_transition(action, reward, obs_)

    def learn(self):
        if self.learn_step_count == self.replace_target_iter:
            self.learn_step_count = 0
            self.q_next.load_state_dict(self.q_eval.state_dict())
        if self.learn_step == self.save_step:
            torch.save(self.q_eval.state_dict(), save_q_eval_path)
            torch.save(self.q_next.state_dict(), save_q_next_path)
            self.learn_step = 0
        batch_leaf_index, IS_weights, batch_obs, batch_action, batch_reward, batch_obs_ = self.memory.get_memory(
            self.batch_size)
        batch_obs = Variable(torch.from_numpy(batch_obs).type(dtype))
        batch_obs_ = Variable(torch.from_numpy(batch_obs_).type(dtype))
        IS_weights = Variable(torch.from_numpy(IS_weights).type(dtype))
        q_next = self.q_next(batch_obs_ / 255.0)
        q_eval = self.q_eval(batch_obs / 255.0)
        reward_batch = torch.from_numpy(batch_reward)
        if USE_CUDA:
            reward_batch = reward_batch.cuda()

        q_target = q_eval.clone().detach()
        batch_index = np.arange(self.batch_size)
        q_target[batch_index, batch_action] = reward_batch.float() + self.gamma * torch.max(q_next, dim=1)[0]
        loss = self.loss_func(IS_weights, q_eval, q_target)
        abs_error_loss = self.abs_errors_func(q_target, q_eval)
        self.memory.batch_update(batch_leaf_index, abs_error_loss.detach().cpu())
        self.op.zero_grad()
        loss.backward()
        self.op.step()

        self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon < self.epsilon_max else self.epsilon_max
        self.learn_step_count += 1
        self.learn_step += 1

    def store_start_frame(self, obs):
        self.memory.store_frame(obs)

    def get_last_memory(self):
        return self.memory.get_last_frame()

    def double_learn(self):
        if self.learn_step_count == self.replace_target_iter:
            self.learn_step_count = 0
            self.q_next.load_state_dict(self.q_eval.state_dict())
        if self.learn_step == self.save_step:
            torch.save(self.q_eval.state_dict(), save_q_eval_path)
            torch.save(self.q_next.state_dict(), save_q_next_path)
            self.learn_step = 0
        batch_leaf_index, IS_weights, batch_obs, batch_action, batch_reward, batch_obs_ = self.memory.get_memory(
            self.batch_size)
        batch_obs = Variable(torch.from_numpy(batch_obs).type(dtype))
        batch_obs_ = Variable(torch.from_numpy(batch_obs_).type(dtype))
        IS_weights = Variable(torch.from_numpy(IS_weights).type(dtype))
        q_next = self.q_next(batch_obs_ / 255.0)
        q_eval_next = self.q_eval(batch_obs_ / 255.0)
        q_eval = self.q_eval(batch_obs / 255.0)
        reward_batch = torch.from_numpy(batch_reward)
        if USE_CUDA:
            reward_batch = reward_batch.cuda()

        q_target = q_eval.clone().detach()
        batch_index = np.arange(self.batch_size)
        max_act_q_eval_next = torch.argmax(q_eval_next, dim=1)
        select_q_next = q_next[batch_index, max_act_q_eval_next]
        q_target[batch_index, batch_action] = reward_batch.float() + self.gamma * select_q_next
        loss = self.loss_func(IS_weights, q_eval, q_target)
        abs_error_loss = self.abs_errors_func(q_target, q_eval)
        self.memory.batch_update(batch_leaf_index, abs_error_loss.detach().cpu())
        self.op.zero_grad()
        loss.backward()
        self.op.step()

        self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon < self.epsilon_max else self.epsilon_max
        self.learn_step_count += 1
        self.learn_step += 1
    
\end{lstlisting}
\section{网络模型}
\begin{lstlisting}[language={python}]
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @File  : model.py
# @Author: zixiao
# @Date  : 2019-04-01
# @Desc  :
import torch
import torch.nn as nn


class DuelingCNN(nn.Module):
    def __init__(self, in_channels, num_action):
        super(DuelingCNN, self).__init__()
        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2)

        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2)
        )
        self.conv3 = nn.Sequential(
            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=2, stride=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2)
        )
        self.Advantage_fc4 = nn.Linear(in_features=2 * 2 * 128, out_features=512)
        self.Advantage_fc5 = nn.Linear(in_features=512, out_features=num_action)

        self.Value_fc4 = nn.Linear(in_features=2 * 2 * 128, out_features=128)
        self.Value_fc5 = nn.Linear(in_features=128, out_features=1)

    def forward(self, x):  # 8 200 200
        x = self.conv1(x)  # 32 24,24
        x = self.conv2(x)  # 64 5 5
        x = self.conv3(x)  # 128 2 2
        A = self.Advantage_fc4(x.view(x.size(0), -1))
        A = self.Advantage_fc5(A)
        V = self.Value_fc4(x.view(x.size(0), -1))
        V = self.Value_fc5(V)
        A_mean = torch.mean(A, dim=1, keepdim=True)
        return V + (A - A_mean)


class CNN(nn.Module):
    def __init__(self, in_channels, num_action):
        super(CNN, self).__init__()
        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2)

        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2)
        )
        self.conv3 = nn.Sequential(
            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=2, stride=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2)
        )
        self.fc4 = nn.Linear(in_features=2 * 2 * 128, out_features=256)
        self.fc5 = nn.Linear(in_features=256, out_features=num_action)

    def forward(self, x):  # 8 200 200
        x = self.conv1(x)  # 32 24,24
        x = self.conv2(x)  # 64 5 5
        x = self.conv3(x)  # 128 2 2
        x = self.fc4(x.view(x.size(0), -1))
        x = self.fc5(x)
        return x
    
\end{lstlisting}
\section{经验库}
\begin{lstlisting}[language={python}]
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @File  : memory.py
# @Author: zixiao
# @Date  : 2019-04-01
# @Desc  :
import numpy as np


class SumTree(object):
    data_index = 0

    def __init__(self, size, frame_len, w, h):
        self.data_size = size
        self.tree_size = 2 * size - 1
        self.tree = np.zeros(self.tree_size,dtype=np.float32)
        self.data_obs = np.zeros((size, w, h), dtype=np.uint8)
        self.data_reward = np.zeros(size, dtype=np.float32)
        self.data_action = np.zeros(size, dtype=np.uint8)
        self.frame_len = frame_len
        self.num_data = 0
        self.data_count = 0

    def add(self, tree_point, action, reward, obs_):
        tree_index = self.data_index + self.data_size - 1
        self.data_action[self.data_index] = action
        self.data_reward[self.data_index] = reward
        self.data_index = int((self.data_index + 1) % self.data_size)
        self.data_obs[self.data_index] = obs_
        self.update(tree_index, tree_point)
        self.data_count += 1
        self.num_data = min(self.data_size, self.data_count)

    def update(self, tree_index, pointer):
        change = pointer - self.tree[tree_index]
        self.tree[tree_index] = pointer
        while tree_index != 0:
            tree_index = (tree_index - 1) // 2
            self.tree[tree_index] += change

    @property
    def total_weight(self):
        return self.tree[0]

    def get_leaf(self, value):
        parent_index = 0
        while True:
            left_index = 2 * parent_index + 1
            right_index = left_index + 1
            if left_index >= self.tree_size:
                break
            else:
                if value <= self.tree[left_index]:
                    parent_index = left_index
                else:
                    value -= self.tree[left_index]
                    parent_index = right_index
        leaf_index = parent_index
        
        data_index = leaf_index - (self.data_size - 1)
        
        obs_frame, obs_frame_ = self.get_frame(data_index)

        return leaf_index, self.tree[leaf_index], obs_frame, self.data_action[data_index], \
                self.data_reward[data_index], obs_frame_

    def store_obs(self, obs):
        self.data_obs[self.data_index] = obs

    def get_last_frame(self):
        start = self.data_index - self.frame_len + 1
        end = self.data_index
        if start < 0:
            start += self.num_data
            obs_frame = np.concatenate((self.data_obs[start:self.num_data],
                                        self.data_obs[0:end + 1]))
        else:
            obs_frame = self.data_obs[start:end + 1]
        return obs_frame

    def get_frame(self, data_index):
        obs_start = data_index - self.frame_len + 1
        obs_end = data_index
        obs_start_ = int((data_index + 1) % self.num_data)
        obs_end_ = obs_start_ + self.frame_len - 1
        if obs_start < 0:
            obs_start += self.num_data
            obs_frame = np.concatenate((self.data_obs[obs_start:self.num_data], self.data_obs[0:obs_end + 1]))
        else:
            obs_frame = self.data_obs[obs_start:obs_end + 1]
        if obs_end_ >= self.num_data:
            obs_end_ -= self.num_data
            obs_frame_ = np.concatenate((self.data_obs[obs_start_:self.num_data], self.data_obs[0:obs_end_ + 1]))
        else:
            obs_frame_ = self.data_obs[obs_start_:obs_end_ + 1]
        
        return obs_frame, obs_frame_


class Memory(object):
    epsilon = 0.01
    alpha = 0.6
    beta = 0.4
    beta_increment_per_sampling = 0.001
    abs_err_upper = 1

    def __init__(self, size, frame_len, w, h):
        self.size = size
        self.frame_len = frame_len
        self.w = w
        self.h = h
        self.tree = SumTree(size=self.size, frame_len=self.frame_len, w=self.w, h=self.h)

    def store_transition(self, action, reward, obs_):
        max_leaf_weight = np.max(self.tree.tree[-self.tree.data_size:])
        if max_leaf_weight == 0:
            max_leaf_weight = self.abs_err_upper
        self.tree.add(max_leaf_weight, action, reward, obs_)

    def get_memory(self, batch_size):
        batch_leaf_index = np.zeros(batch_size, dtype=np.int32)
        batch_action = np.zeros(batch_size, dtype=np.uint8)
        batch_reward = np.zeros(batch_size, dtype=np.float32)
        batch_obs = np.zeros((batch_size, self.frame_len, self.w, self.h), dtype=np.uint8)
        batch_obs_ = np.zeros((batch_size, self.frame_len, self.w, self.h), dtype=np.uint8)
        IS_weights = np.zeros((batch_size, 1))

        priority_segment = self.tree.total_weight / batch_size
        print('total_weight: ', self.tree.total_weight)
        self.beta = np.min([1, self.beta + self.beta_increment_per_sampling])
        end = self.tree.data_size + self.tree.num_data - 1
        min_probability = np.min(self.tree.tree[-self.tree.data_size:end]) / self.tree.total_weight
        values = []
        leafs = []
        leaf_values = []
        for i in range(batch_size):
            low = priority_segment * i
            high = priority_segment * (i + 1)
            value = np.random.uniform(low, high)
            leaf_index, leaf_value, obs, action, reward, obs_ = self.tree.get_leaf(value)
            probability = leaf_value / self.tree.total_weight

            IS_weights[i, 0] = np.power(probability / min_probability, -self.beta)
            batch_leaf_index[i] = leaf_index

            values.append(value)
            leafs.append(leaf_index)
            leaf_values.append(leaf_value)

            batch_obs[i] = obs
            batch_obs_[i] = obs_
            batch_action[i] = action
            batch_reward[i] = reward
        # print(values)
        # print(leafs)
        print(leaf_values)
        return batch_leaf_index, IS_weights, batch_obs, batch_action, batch_reward, batch_obs_

    def batch_update(self, tree_index, abs_errors):
        abs_errors += self.epsilon
        clipped_errors = np.minimum(abs_errors, self.abs_err_upper)
        ps = np.power(clipped_errors, self.alpha)
        for t_index, p in zip(tree_index, ps):
            self.tree.update(t_index, p)

    def store_frame(self, obs):
        self.tree.store_obs(obs)

    def get_last_frame(self):
        return self.tree.get_last_frame()
    
\end{lstlisting}