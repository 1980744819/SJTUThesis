\chapter*{\zihao{-3}\textbf{前言}}
\label{chap:introduce}
深度强化学习(deep reinforcement learning:DRL)结合了深度神经网络和强化学习的优势，可以解决机器在复杂的高维度状态空间中的感知和决策问题。在游戏领域，无人驾驶，推荐系统，机器人等领域，深度强化学习已经取得了突破性进展\cite{唐振韬2017深度强化学习进展,赵冬斌2016深度强化学习综述：兼论计算机围棋的发展,Li2017Deep,arulkumaran2017brief}。强化学习属于无监督学习，强化学习与其他机器学习任务的显著区别在于：没有预先给出训练数据，而是要通过与环境的交互产生；在环境中执行一个动作，没有关于这个动作的好坏标记，而只有在交互一段时间后才能得知累积奖励，从而推断出之前动作的好坏。强化学习任务通常使用马尔科夫决策过程（Markov Decision Process，MDP）来描述，具体来说：机器处在一个环境中，机器对当前环境的感知就是当前的状态；机器通过执行不同的动作来影响环境，在机器执行完成一个动作之后，当前环境的状态会根据某个概率转移成为另一个状态；在同一时刻，环境会根据背后已经规定好的的奖励函数反馈给机器一个奖励或惩罚。总的来说，强化学习主要包含：环境的状态、机器可以选择的动作、机器从当前状态转移到下一个状态的转移概率、环境在机器执行动作后反馈的奖赏函数四个要素\cite{2011机器学习及其应用}。

在人工智能这个领域，衡量智能的关键指标是机器的感知和决策能力。伴随着近几年强化学习和深度学习的发展，直接根据原始数据来提取的高水平特征并进行感知决策不再困难\cite{唐振韬2017深度强化学习进展}。深度强化学习在最近短短几年的时间里取得了很多进展，同时也在机器学习这个领域中得到了很多学者的关注。传统强化学习的局限性在于动作和样本的空间都很小，而且一般是离散的场景。在实际的任务中往往要求高维的状态空间和连续的空间动作，如声音、图像原始高维数据的输入，传统强化学习无法处理这样非常复杂的场景。前些年开始兴起的深度学习，刚好可以对应高维的输入，如果能将两者结合起来，那么机器将会同时拥有深度学习强大的理解能力和强化学习优秀的决策能力。

在本课题中，我们将使用深度强化学习的经典算法DQN及其三大改进算法控制经典游戏超级玛丽运动到距离起点更远的距离。主要的难点在于超级玛丽相比于其他Atari游戏，控制更加困难，逻辑更加复杂，游戏中还会遇到NPC（非玩家控制角色）与Agent的交互。

本论文由以下几个部分构成：第一章是针对深度强化学习的发展以及研究现状的概论；第二章重点讲解DQN算法原理，以及DQN算法三个重大改进的原理；第三章是整个课题的实验实施设计，包括游戏控制环境，逻辑流程，神经网络的整体结构，控制算法的工作流程；第四章是对实验结果的分析报告，包括实验结果的数据情况，结论，以及分析未来需要进行的改进措施；第五章是对本次课题做的总结，分析存在的问题，实验效果，改进方向等。