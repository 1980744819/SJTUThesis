@article{赵冬斌2016深度强化学习综述：兼论计算机围棋的发展,
  title={深度强化学习综述：兼论计算机围棋的发展},
  author={赵冬斌 and 邵坤 and 朱圆恒 and 李栋 and 陈亚冉 and 王海涛 and 刘德荣 and 周彤 and 王成红},
  journal={控制理论与应用},
  volume={33},
  number={6},
  pages={701-717},
  year={2016},
 keywords={深度强化学习;初弈号;深度学习;强化学习;人工智能},
 abstract={深度强化学习将深度学习的感知能力和强化学习的决策能力相结合,可以直接根据输入的图像进行控制,是一种更接近人类思维方式的人工智能方法.自提出以来,深度强化学习在理论和应用方面均取得了显著的成果.尤其是谷歌深智(Deep Mind)团队基于深度强化学习方法研发的计算机围棋"初弈号–Alpha Go",在2016年3月以4:1的大比分战胜了世界围棋顶级选手李世石(Lee Sedol),成为人工智能历史上一个新里程碑.为此,本文综述深度强化学习的发展历程,兼论计算机围棋的历史,分析算法特性,探讨未来的发展趋势和应用前景,期望能为控制理论与应用新方向的发展提供有价值的参考.},
}

@article{Li2017Deep,
  title={Deep Reinforcement Learning: An Overview},
  author={Li, Yuxi},
  year={2017},
 keywords={Computer Science - Learning},
 abstract={Abstract: We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions.},
}

@article{唐振韬2017深度强化学习进展,
  title={深度强化学习进展:从AlphaGo到AlphaGo Zero},
  author={唐振韬 and 邵坤 and 赵冬斌 and 朱圆恒},
  journal={控制理论与应用},
  volume={34},
  number={12},
  year={2017},
 keywords={深度强化学习;AlphaGoZero;深度学习;强化学习;人工智能},
 abstract={2016年初,AlphaGo战胜李世石成为人工智能的里程碑事件.其核心技术深度强化学习受到人们的广泛关注和研究,取得了丰硕的理论和应用成果.并进一步研发出算法形式更为简洁的AlphaGo Zero,其采用完全不基于人类经验的自学习算法,完胜AlphaGo,再一次刷新人们对深度强化学习的认知.深度强化学习结合了深度学习和强化学习的优势,可以在复杂高维的状态动作空间中进行端到端的感知决策.本文主要介绍了从AlphaGo到Alpha-Go Zero的深度强化学习的研究进展.首先回顾对深度强化学习的成功作出突出贡献的主要算法,包括深度Q网络算法、A3C算法、策略梯度算法及其他算法的相应扩展.然后给出AlphaGo Zero的详细介绍和讨论,分析其对人工智能的巨大推动作用.并介绍了深度强化学习在游戏、机器人、自然语言处理、智能驾驶、智能医疗等领域的应用进展,以及相关资源进展.最后探讨了深度强化学习的发展展望,以及对其他潜在领域的人工智能发展的启发意义.},
}

@article{Mnih2013Playing,
  title={Playing Atari with Deep Reinforcement Learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={Computer Science},
  year={2013},
 keywords={Computer Science - Learning},
 abstract={Abstract: We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
}

@article{Watkins1992Technical,
  title={Technical Note: Q-Learning},
  author={Watkins, Christopher J. C. H. and Dayan, Peter},
  journal={Machine Learning},
  volume={8},
  number={3-4},
  pages={279-292},
  year={1992},
 keywords={mathcal{Q}$$ -learning;reinforcement learning;temporal differences;asynchronous dynamic programming},
 abstract={\(\mathcal{Q}\) -learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.This paper presents and proves in detail a convergence theorem for \(\mathcal{Q}\) -learning based on that outlined in Watkins (1989). We show that \(\mathcal{Q}\) -learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where many \(\mathcal{Q}\) values can be changed each iteration, rather than just one.},
}

@article{Lillicrap2015Continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={Computer Science},
  volume={8},
  number={6},
  pages={A187},
  year={2015},
 keywords={Computer Science - Learning;Statistics - Machine Learning},
 abstract={We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
}

@article{Barto1998Reinforcement,
  title={Reinforcement Learning: An Introduction},
  author={Barto, Andrew G and Sutton, Richard S},
  year={1998},
}

@misc{gym-super-mario-bros,
  author = {Christian Kauten},
  title = {{S}uper {M}ario {B}ros for {O}pen{AI} {G}ym},
  year = {2018},
  publisher = {GitHub},
  howpublished = {\url{https://github.com/Kautenja/gym-super-mario-bros}},
}
@inproceedings{zeiler2014visualizing,
  title={Visualizing and understanding convolutional networks},
  author={Zeiler, Matthew D and Fergus, Rob},
  booktitle={European conference on computer vision},
  pages={818--833},
  year={2014},
  organization={Springer}
}
@article{陈兴国2016强化学习及其在电脑围棋中的应用,
  title={强化学习及其在电脑围棋中的应用},
  author={陈兴国 and 俞扬},
  journal={自动化学报},
  volume={42},
  number={5},
  pages={685--695},
  year={2016}
}
@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484},
  year={2016},
  publisher={Nature Publishing Group}
}
@article{li2017deep,
  title={Deep speaker: an end-to-end neural speaker embedding system},
  author={Li, Chao and Ma, Xiaokong and Jiang, Bing and Li, Xiangang and Zhang, Xuewei and Liu, Xiao and Cao, Ying and Kannan, Ajay and Zhu, Zhenyao},
  journal={arXiv preprint arXiv:1705.02304},
  year={2017}
}
@article{arulkumaran2017brief,
  title={A brief survey of deep reinforcement learning},
  author={Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  journal={arXiv preprint arXiv:1708.05866},
  year={2017}
}
@article{常亮2016图像理解中的卷积神经网络,
  title={图像理解中的卷积神经网络},
  author={常亮 and 邓小明 and 周明全 and 武仲科 and 袁野 and 杨硕 and 王宏安},
  journal={自动化学报},
  volume={42},
  number={9},
  pages={1300--1312},
  year={2016}
}
@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}
@article{wang2016sample,
  title={Sample efficient actor-critic with experience replay},
  author={Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and de Freitas, Nando},
  journal={arXiv preprint arXiv:1611.01224},
  year={2016}
}
@article{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}
@inproceedings{van2016deep,
  title={Deep reinforcement learning with double q-learning},
  author={Van Hasselt, Hado and Guez, Arthur and Silver, David},
  booktitle={Thirtieth AAAI Conference on Artificial Intelligence},
  year={2016}
}
@inproceedings{hasselt2010double,
  title={Double Q-learning},
  author={Hasselt, Hado V},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2613--2621},
  year={2010}
}
@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}
@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick and others},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Taipei, Taiwan}
}
@article{schaul2015prioritized,
  title={Prioritized experience replay},
  author={Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  journal={arXiv preprint arXiv:1511.05952},
  year={2015}
}
@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}
@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016}
}
@article{heess2017emergence,
  title={Emergence of locomotion behaviours in rich environments},
  author={Heess, Nicolas and Sriram, Srinivasan and Lemmon, Jay and Merel, Josh and Wayne, Greg and Tassa, Yuval and Erez, Tom and Wang, Ziyu and Eslami, SM and Riedmiller, Martin and others},
  journal={arXiv preprint arXiv:1707.02286},
  year={2017}
}
@book{2011机器学习及其应用,
  title={机器学习及其应用2009},
  year={2011},
}
@article{Rosenblatt1958The,
  title={The perceptron: a probabilistic model for information storage and organization in the brain.},
  author={Rosenblatt, F},
  journal={Psychological Review},
  volume={65},
  number={6},
  pages={386-408},
  year={1958},
}
@article{rumelhart1988learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J and others},
  journal={Cognitive modeling},
  volume={5},
  number={3},
  pages={1},
  year={1988}
}
@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick and others},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Taipei, Taiwan}
}
@article{hinton2006fast,
  title={A fast learning algorithm for deep belief nets},
  author={Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
  journal={Neural computation},
  volume={18},
  number={7},
  pages={1527--1554},
  year={2006},
  publisher={MIT Press}
}
@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}
@article{nair2015massively,
  title={Massively parallel methods for deep reinforcement learning},
  author={Nair, Arun and Srinivasan, Praveen and Blackwell, Sam and Alcicek, Cagdas and Fearon, Rory and De Maria, Alessandro and Panneershelvam, Vedavyas and Suleyman, Mustafa and Beattie, Charles and Petersen, Stig and others},
  journal={arXiv preprint arXiv:1507.04296},
  year={2015}
}
@article{wang2015dueling,
  title={Dueling network architectures for deep reinforcement learning},
  author={Wang, Ziyu and Schaul, Tom and Hessel, Matteo and Van Hasselt, Hado and Lanctot, Marc and De Freitas, Nando},
  journal={arXiv preprint arXiv:1511.06581},
  year={2015}
}
@inproceedings{osband2016deep,
  title={Deep exploration via bootstrapped DQN},
  author={Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
  booktitle={Advances in neural information processing systems},
  pages={4026--4034},
  year={2016}
}
@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016}
}
@inproceedings{sutton2000policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  booktitle={Advances in neural information processing systems},
  pages={1057--1063},
  year={2000}
}
@article{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}
@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529},
  year={2015},
  publisher={Nature Publishing Group}
}
@article{hinton2006reducing,
  title={Reducing the dimensionality of data with neural networks},
  author={Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
  journal={science},
  volume={313},
  number={5786},
  pages={504--507},
  year={2006},
  publisher={American Association for the Advancement of Science}
}
@Misc{CNN2017,
howpublished = {\url{http://timmurphy.org/2009/07/22/line-spacing-in-latex-documents/}},
note = {Accessed April 4, 2010},
title = {Line Spacing in LaTeX documents},
author = {Murphy, Timothy I}
}
@misc{CNN2017,
  howpublished = {\url{https://www.cnblogs.com/skyfsm/p/6790245.html}},
  title = {卷积神经网络CNN总结},
  author = {Madcola},
  year = 2017,
}

